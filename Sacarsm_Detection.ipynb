{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sacarsm_Detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2QxiIY5Hk9cFIsRLACUly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buixuanthanh96/CS114.K21/blob/master/Sacarsm_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j54Avohpiv7",
        "colab_type": "text"
      },
      "source": [
        "Sinh viên thực hiện: Bùi Xuân Thành\n",
        "MSSV:14520830"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEMMYh6Mpkrq",
        "colab_type": "text"
      },
      "source": [
        "1. **Mô tả bài toán và thu thập dữ liệu**:\n",
        "- Sacarsm detection là bài toán phát hiện trào phúng, châm biếm.\n",
        "- Dữ liệu bài toán được thu tập từ 2 nguồn trang web tin tức là: The Onion là một tờ báo trào phúng của Mỹ thường viết những bài báo mang tính châm biếm, trào phúng, tường thuật các tin tức quốc tế và HuffPost là một trang tin tức trực tuyến và log về tin tức và ý kiến của Mỹ.\n",
        "- Link nguồn lấy dữ liệu:  https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/download\n",
        "- Tập dữ liệu có tổng cộng 26709 mẫu trong đó có 11724 mẫu châm biếm và 14985 mẫu không châm biếm.\n",
        "- Mỗi mẫu dữ liệu gồn 3 thuộc tính :\n",
        "•\tarticle_link: Liên kết với bài viết tin tức.\n",
        "•\theadline: tiêu đề bài báo.\n",
        "•\tis_sarcastic : bài báo có châm biếm hay không(1 là châm chiếm và 0 không châm biếm).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0Bh8xIppo5K",
        "colab_type": "text"
      },
      "source": [
        "2. **Thu thập 2000 Headlines mới**\n",
        "- Dữ liệu mới gồm 1000 Headlines được thu thập từ trang báo châm biếm theonion và 1000 Headlines được thu thập từ trang báo huffpost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kknQw9qypq8K",
        "colab_type": "text"
      },
      "source": [
        "3. **Mô tả cách xử lý dữ liệu, feature engineering trên dataset đã cho**\n",
        "- Dữ liệu đầu tiên được xử lý loại bỏ dấu câu và số.\n",
        "- Sau đó được xử lý loại bỏ stopword\n",
        "- Kế tiếp tách Headline thành các từ và chuyển các từ về dạng nguyên mẫu, kí tự thường\n",
        "- Feature Engineering: dùng TF-IDF Vectors as features để đưa dữ liệu dạng văn bản đã được xử lý về dạng vector thuộc tính có dạng số học.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKGij1bLps5i",
        "colab_type": "text"
      },
      "source": [
        "4. **Mô tả quá trình chọn model, học và fine tuning**\n",
        "- Chọn Naive Bayes vì thường được sử dụng trong các bài toán Text Classification có khả năng học tập và dự đoán hiệu quả cao, nhanh và có khả năng mở rộng cao (hoạt động tốt với dữ liệu nhiều chiều). \n",
        "- Dữ liệu được chia thành train/test theo tỉ lệ 80/20.\n",
        "fine tuning : alpha=1.0, fit_prior=True, class_prior=None\n",
        "- Chạy SVM để so sánh thấy Naive Bayes nhỉnh hơn so với svm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryN5gjymuo45",
        "colab_type": "text"
      },
      "source": [
        "6.**Đối chiếu performance của model trên dataset đã cho và trên 2000 headine mới. Nêu nhận xét của nhóm về bài toán này.**\n",
        "- Accuracy chạy trên dữ liệu cũ của cả 2 model đều lớn hơn accuracy chạy trên bộ dữ liệu test mới.\n",
        "=> dữ liệu thu thập ảnh hưởng rất lớn đến performance của model\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2kBBzQXpeEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "3dd83344-bf0f-423f-dff4-fef28800f9ab"
      },
      "source": [
        "#Lấy dữ liệu từ git xuống\n",
        "#Sarcasm_Headlines_Dataset.json : dữ liệu mẫu có sẵn\n",
        "# new_Sarcasm_Headlines_Dataset.json : là 2000 headlines mới tự thu thập\n",
        "!git clone https://github.com/buixuanthanh96/SacarsmDetection.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SacarsmDetection'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 20 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21k7w7tWpxih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "381449ee-df14-4896-81fb-8e31c8b00663"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAeoNPl6p0f5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#xử lý dữ liệu có sẵn\n",
        "import pandas as pd\n",
        "import string, re, nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def getPOS(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()#xác định loại từ\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN) #trả về dưới dạng danh từ\n",
        "\n",
        "path = \"SacarsmDetection/input/\"\n",
        "data = pd.read_json(path + 'Sarcasm_Headlines_Dataset.json', lines=True)\n",
        "data = data.drop(columns = [\"article_link\"])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "headlinesPP = []\n",
        "for index, row in data.iterrows():\n",
        "    row[\"headline\"] = row[\"headline\"].translate(str.maketrans('', '', string.punctuation)) #Loại bỏ dấu câu\n",
        "    row[\"headline\"] = row[\"headline\"].translate(str.maketrans('', '', string.digits)) #Loại bỏ kí tự số\n",
        "    row[\"headline\"] = ' '.join([word for word in row[\"headline\"].split() if word not in (stopwords.words('english'))])#loại bỏ stopwords\n",
        "    tokenList = re.sub(\"[^\\S]\", \" \",  row[\"headline\"]).split() #tách chuỗi thành các từ\n",
        "    #tokenList = nltk.word_tokenize(row[\"headline\"])\n",
        "    for i in range(0, len(tokenList)): #lemmatization\n",
        "        tokenList[i] = lemmatizer.lemmatize(tokenList[i], getPOS(tokenList[i]))#chuyển từ về dạng nguyên mẫu\n",
        "    headlinesPP.append(tokenList)\n",
        "dataPP = pd.DataFrame(data = {'headline': headlinesPP, 'is_sarcastic': data[\"is_sarcastic\"]} )\n",
        "dataPP.to_csv(path+'dataPostProcessed.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a9cDykfp2ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#xử lý 2000 mẫu dữ liệu tự thu thập\n",
        "import pandas as pd\n",
        "import string, re, nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def getPOS(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()#xác định loại từ\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN) #trả về dưới dạng danh từ\n",
        "\n",
        "path = \"SacarsmDetection/input/\"\n",
        "data = pd.read_json(path + 'new_Sarcasm_Headlines_Dataset.json', lines=True)\n",
        "data = data.drop(columns = [\"article_link\"])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "headlinesPP = []\n",
        "for index, row in data.iterrows():\n",
        "    row[\"headline\"] = row[\"headline\"].translate(str.maketrans('', '', string.punctuation)) #Loại bỏ dấu câu\n",
        "    row[\"headline\"] = row[\"headline\"].translate(str.maketrans('', '', string.digits)) #Loại bỏ kí tự số\n",
        "    row[\"headline\"] = ' '.join([word for word in row[\"headline\"].split() if word not in (stopwords.words('english'))])#loại bỏ stopwords\n",
        "    tokenList = re.sub(\"[^\\S]\", \" \",  row[\"headline\"]).split() #tách chuỗi thành các từ\n",
        "    #tokenList = nltk.word_tokenize(row[\"headline\"])\n",
        "    for i in range(0, len(tokenList)): #lemmatization\n",
        "        tokenList[i] = lemmatizer.lemmatize(tokenList[i], getPOS(tokenList[i]))#chuyển từ về dạng nguyên mẫu\n",
        "    headlinesPP.append(tokenList)\n",
        "dataPP = pd.DataFrame(data = {'headline': headlinesPP, 'is_sarcastic': data[\"is_sarcastic\"]} )\n",
        "dataPP.to_csv(path+'new_dataPostProcessed.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcRszYWiqJtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "bbfa534b-3bac-478e-d37b-60f6045f15a0"
      },
      "source": [
        "#Chạy model Naive Bayes\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path = \"SacarsmDetection/input/\"\n",
        "data = pd.read_csv(path + 'dataPostProcessed.csv', index_col=0)\n",
        "X = data['headline']#.append(df['headline'])\n",
        "Y = data['is_sarcastic']#.append(df['is_sarcastic'])\n",
        "\n",
        "new_input = pd.read_csv(path + 'new_dataPostProcessed.csv', index_col=0)\n",
        "X_new_test = new_input['headline']#.append(df['headline'])\n",
        "Y_new_test = new_input['is_sarcastic']#.append(df['is_sarcastic'])\n",
        "\n",
        "\n",
        "\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.2)\n",
        "print(\"*** Số mẫu train: \"+str(len(xTrain))+ \", test: \"+str(len(xTest)))\n",
        "classifier = Pipeline([('tfidf', TfidfVectorizer()), ('classifier', MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None))])\n",
        "    \n",
        "classifier.fit(xTrain, yTrain)\n",
        "print(\"###### Naive Bayes #########\")\n",
        "prediction = classifier.predict(xTest)\n",
        "prediction_new = classifier.predict(X_new_test)\n",
        "#print(classification_report(yTest, prediction))\n",
        "print(\"accuracy khi test với dữ liệu cũ: \",metrics.accuracy_score(yTest, prediction))\n",
        "print(\"accuracy khi test với 2000 dữ liệu mới: \",metrics.accuracy_score(Y_new_test, prediction_new))\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Số mẫu train: 21367, test: 5342\n",
            "###### Naive Bayes #########\n",
            "accuracy khi test với dữ liệu cũ:  0.7905278921752152\n",
            "accuracy khi test với 2000 dữ liệu mới:  0.655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Ud4BefqN9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "c07b0f32-7c70-4371-9031-3297b62c1970"
      },
      "source": [
        "#Chạy model SVM\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path = \"SacarsmDetection/input/\"\n",
        "data2 = pd.read_csv(path + 'dataPostProcessed.csv', index_col=0)\n",
        "X = data2['headline']\n",
        "Y = data2['is_sarcastic']\n",
        "\n",
        "new_input = pd.read_csv(path + 'new_dataPostProcessed.csv', index_col=0)\n",
        "X_new_test = new_input['headline']\n",
        "Y_new_test = new_input['is_sarcastic']\n",
        "\n",
        "\n",
        "\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.2)\n",
        "print(\"*** Số mẫu train: \"+str(len(xTrain))+ \", test: \"+str(len(xTest)))\n",
        "\n",
        "classifier2 = Pipeline([('tfidf', TfidfVectorizer()), ('classifier', LinearSVC())])\n",
        " \n",
        "classifier2.fit(xTrain, yTrain)\n",
        "print(\"###### LinearSVC #########\")\n",
        "prediction = classifier2.predict(xTest)\n",
        "prediction_new = classifier2.predict(X_new_test)\n",
        "#print(classification_report(yTest, prediction))\n",
        "print(\"accuracy khi test với dữ liệu cũ: \",metrics.accuracy_score(yTest, prediction))\n",
        "print(\"accuracy khi test với 2000 dữ liệu mới: \",metrics.accuracy_score(Y_new_test, prediction_new))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Số mẫu train: 21367, test: 5342\n",
            "###### LinearSVC #########\n",
            "accuracy khi test với dữ liệu cũ:  0.7813552976413328\n",
            "accuracy khi test với 2000 dữ liệu mới:  0.671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk82dCAJqS-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8eda290d-6233-4db4-a03d-9e286a55b443"
      },
      "source": [
        "print(\"Nhập vào headline:\")\n",
        "headline = str(input())\n",
        "headline = headline.translate(str.maketrans('', '', string.punctuation)) #Loại bỏ dấu câu\n",
        "headline = headline.translate(str.maketrans('', '', string.digits)) #Loại bỏ kí tự số\n",
        "headline = ' '.join([word for word in headline.split() if word not in (stopwords.words('english'))])#loại bỏ stopwords\n",
        "tokenList = re.sub(\"[^\\S]\", \" \",  headline).split() #tách chuỗi thành các từ\n",
        "#tokenList = nltk.word_tokenize(row[\"headline\"])\n",
        "for i in range(0, len(tokenList)): #lemmatization\n",
        "    tokenList[i] = lemmatizer.lemmatize(tokenList[i], getPOS(tokenList[i]))\n",
        "prediction1 = classifier.predict( [headline])\n",
        "print(\"Sacarsm detection : \", prediction1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nhập vào headline:\n",
            "Trump Inadvertently Saves Life Of Yemeni Family After Appropriating Pentagon Money For Border Wall\n",
            "Sacarsm detection :  [1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}